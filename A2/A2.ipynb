{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlimitedDataWorks:\n",
    "\n",
    "    def __init__(self, deg):\n",
    "        self.exp = []\n",
    "        for i in range(deg+1):\n",
    "            for j in range(deg+1):\n",
    "                if i+j <= deg:\n",
    "                    self.exp.append((i, j))\n",
    "\n",
    "    def train_test_split(self, dataframe):\n",
    "        self.data = pd.DataFrame([])\n",
    "        self.count = -1\n",
    "        for (a, b) in self.exp:\n",
    "            self.count += 1\n",
    "            res = ((dataframe[\"lat\"] ** b) * (dataframe[\"lon\"] ** a))\n",
    "            self.data.insert(self.count, \"col\" + str(a) + str(b), res, True)\n",
    "\n",
    "        self.count += 1\n",
    "        normalize = lambda x: ((x - x.min()) / (x.max() - x.min()))\n",
    "        dataframe = normalize(dataframe)\n",
    "        self.data = normalize(self.data)\n",
    "        self.data[\"col00\"] = [1.0]*len(self.data)\n",
    "        \n",
    "        # generate a 70-20-10 split on the data:\n",
    "        X = self.data[:304113]\n",
    "        Y = dataframe[\"alt\"][:304113]\n",
    "        xval = self.data[304113:391088]\n",
    "        yval = dataframe[\"alt\"][304113:391088]\n",
    "        x = self.data[391088:]\n",
    "        y = dataframe[\"alt\"][391088:]   \n",
    "        return (X, Y, xval, yval, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "\n",
    "    def __init__(self, N, X, Y, x, y, xval, yval):\n",
    "        \"\"\"\n",
    "        X :: training data                  (304113 x 3)\n",
    "        x :: testing data                   (43786 x 3)\n",
    "        Y :: training target values         (304113 x 1)\n",
    "        y :: testing target values          (43786 x 1)\n",
    "        xval :: validation data             (86975 x 3)\n",
    "        yval :: validation training data    (86975 X 1)\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(Y)\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.xval = np.array(xval)\n",
    "        self.yval = np.array(yval)\n",
    "\n",
    "    def score(self, weights):\n",
    "        \"\"\"\n",
    "        the following method helps us find the\n",
    "        R2 (R-squared) error of a given training data\n",
    "        wrt the generated weights\n",
    "        \"\"\"\n",
    "        ss_tot = sum(np.square(np.mean(self.y) - self.y))\n",
    "        ss_res = sum(np.square((self.x @ weights) - self.y))\n",
    "        rmse = sqrt(ss_res/len(self.x))\n",
    "        r2 = (1-(ss_res / ss_tot))\n",
    "        return [r2*100, rmse]\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        train till error is almost constant\n",
    "        \"\"\"\n",
    "        lr = 8.75e-7\n",
    "        prev_err, count = 1e10, 0\n",
    "        W = np.random.randn(self.N)\n",
    "        while True:\n",
    "            diff = ((self.X @ W) - self.Y)\n",
    "            err = 0.5 * (diff @ diff)\n",
    "            grad = 0.5 * (self.X.T @ diff)\n",
    "            if count % 500 == 0:\n",
    "                print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                print(\"error = \", err, \"||\", W)\n",
    "                print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "            W -= lr * grad\n",
    "            if abs(prev_err-err) <= 5e-4:\n",
    "                break\n",
    "            prev_err = err\n",
    "            count += 1\n",
    "        print(count, err)\n",
    "        print(W, self.score(W), end=\"\\n\\n\")\n",
    "\n",
    "    def gradient_descent_L1_reg(self):\n",
    "        \"\"\"\n",
    "        attempts a L1 regularization on the data\n",
    "        considering 10% of training data as validation data\n",
    "        \"\"\"\n",
    "        W_fin = np.array([])\n",
    "        lr, l1_fin = 8e-7, 0\n",
    "        MVLE = 1e10\n",
    "        L1_vals = np.linspace(0.0, 1.0, 11)\n",
    "        sgn = lambda x: (x / abs(x))\n",
    "        for l1 in L1_vals:\n",
    "            prev_err, count = 1e10, 0\n",
    "            W = np.random.randn(self.N)\n",
    "            while True:\n",
    "                diff = ((self.X @ W) - self.Y)\n",
    "                err = 0.5 * ((diff @ diff) + l1*sum([abs(w) for w in W]))\n",
    "                if count % 500 == 0:\n",
    "                    print(\"L1 hyperparamter =\", l1, end=\", \")\n",
    "                    print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                    print(\"error = \", err, \"||\", W)\n",
    "                    print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "                sgn_w = np.array([sgn(w) for w in W])\n",
    "                W -= lr * ((self.X.T @ diff) + 0.5*l1*sgn_w)\n",
    "                if abs(prev_err-err) <= 0.01:\n",
    "                    break\n",
    "                prev_err = err\n",
    "                count += 1\n",
    "            VLD = ((self.xval @ W) - self.yval)\n",
    "            VLE = 0.5 * ((VLD.T @ VLD) + l1*sum([abs(w) for w in W]))\n",
    "            if VLE < MVLE:\n",
    "                W_fin = W\n",
    "                l1_fin = l1\n",
    "                MVLE = VLE\n",
    "        print(MVLE, l1_fin, W_fin)\n",
    "\n",
    "    def gradient_descent_L2_reg(self):\n",
    "        \"\"\"\n",
    "        attempts a L2 regularization on the data\n",
    "        considering 10% of training data as validation data\n",
    "        \"\"\"\n",
    "        W_fin = np.array([])\n",
    "        lr, l2_fin = 5e-7, 0\n",
    "        MVLE = 1e10\n",
    "        L2_vals = np.linspace(0.0, 1.0, 11)\n",
    "        for l2 in L2_vals:\n",
    "            prev_err, count = 1e10, 0\n",
    "            W = np.random.randn(self.N)\n",
    "            while True:\n",
    "                diff = ((self.X @ W) - self.Y)\n",
    "                err = 0.5 * ((diff @ diff) + l2*sum([w*w for w in W]))\n",
    "                if count % 500 == 0:\n",
    "                    print(\"L2 hyperparamter =\", l2, end=\", \")\n",
    "                    print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                    print(\"error = \", err, \"||\", W)\n",
    "                    print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "                W -= lr * ((self.X.T @ diff) + l2*W)\n",
    "                if abs(prev_err-err) <= 0.01:\n",
    "                    break\n",
    "                prev_err = err\n",
    "                count += 1\n",
    "            VLD = ((self.xval @ W) - self.yval)\n",
    "            VLE = 0.5 * ((VLD.T @ VLD) + l2 * (W.T @ W))\n",
    "            if VLE < MVLE:\n",
    "                W_fin = W\n",
    "                l2_fin = l2\n",
    "                MVLE = VLE\n",
    "        print(MVLE, l2_fin, W_fin)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        solves for optimal weights using system of\n",
    "        N linear equations; AW = B, hence, W = inv(A)*B\n",
    "        \"\"\"\n",
    "        B = self.X.T @ self.Y\n",
    "        A = self.X.T @ self.X\n",
    "        W = (np.linalg.inv(A)) @ B\n",
    "        print(W, self.score(W))\n",
    "        tmp = ((self.X @ W) - self.Y)\n",
    "        print(\"train_error =\", 0.5 * (tmp @ tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Degree of the Polynomial:6\n",
      "[ 1.28157283e+02  4.25047105e+05 -2.48499973e+06  5.29818476e+06\n",
      " -5.86510895e+06  3.26349377e+06 -6.24052203e+05  4.39269199e+05\n",
      "  1.25620547e+04  3.41928625e+05 -1.64085375e+05 -1.22191844e+05\n",
      " -2.19000031e+05 -7.32193547e+05  1.25374592e+05 -2.97474281e+05\n",
      "  5.96004469e+05  7.33931250e+03 -1.07784406e+05 -3.99701152e+05\n",
      " -2.96484266e+05  1.89696812e+05  3.19045656e+05  4.81178453e+05\n",
      " -2.27818461e+05  4.16299938e+05 -3.25788516e+04 -3.46609850e+05] [-57.15350550914704, 0.14637633170454326]\n",
      "train_error = 2886.0601486641854\n",
      "epoch = 0 | err_diff = 9999340007.56213\n",
      "error =  659992.4378699014 || [ 0.17139107 -0.34021537 -0.30301937  1.16708612  0.07123813 -0.50763413\n",
      "  0.26731105  0.5664501  -0.60055334 -0.48231611  0.06966841 -0.63224193\n",
      "  0.2275899  -0.17062913  1.35600235  0.26219095  0.06626893 -2.22402218\n",
      "  1.35395193 -0.31663364 -0.18061404  1.0128096  -1.47593055 -1.82005334\n",
      "  0.56233162 -0.72919256 -0.72266932 -0.45917789]\n",
      "score = [-33225.99555014745, 2.1315738747453215]\n",
      "\n",
      "epoch = 500 | err_diff = 0.04627513228024327\n",
      "error =  2593.159067396762 || [ 0.20546481 -0.17983332 -0.15601126  1.29979369  0.18896052 -0.40532911\n",
      "  0.35401672  0.74827788 -0.43507284 -0.33251478  0.20434725 -0.51298449\n",
      "  0.33115278  0.01143662  1.52629173  0.41472673  0.20290055 -2.10323681\n",
      "  1.53624923 -0.14178225 -0.02540015  1.15137404 -1.29340815 -1.64086016\n",
      "  0.72017066 -0.54645156 -0.5393316  -0.27622478]\n",
      "score = [0.6311931803509085, 0.11639487553907266]\n",
      "\n",
      "epoch = 1000 | err_diff = 0.03482755576442287\n",
      "error =  2573.025099409392 || [ 0.19564092 -0.16239435 -0.14821359  1.29756509  0.17648794 -0.42809514\n",
      "  0.3210688   0.74529027 -0.41868518 -0.32518727  0.20184387 -0.5256339\n",
      "  0.30826971  0.00890899  1.54175549  0.42162809  0.2001465  -2.11604754\n",
      "  1.53417793 -0.12712612 -0.01888365  1.14839225 -1.2950268  -1.62690744\n",
      "  0.72634159 -0.54762128 -0.52598859 -0.27694934]\n",
      "score = [2.168334244540815, 0.115491107721334]\n",
      "\n",
      "epoch = 1500 | err_diff = 0.02622168736115782\n",
      "error =  2557.869254414614 || [ 0.18714969 -0.14747566 -0.14152734  1.29562054  0.16565948 -0.44791205\n",
      "  0.29230328  0.7425178  -0.40462357 -0.3188539   0.19971607 -0.53656085\n",
      "  0.28840554  0.00660445  1.55507486  0.42764897  0.19785784 -2.12705848\n",
      "  1.53233766 -0.11444431 -0.01313731  1.14596399 -1.2964065  -1.61476937\n",
      "  0.73184942 -0.54854413 -0.5143099  -0.27741912]\n",
      "score = [3.3595094379019863, 0.11478585822069985]\n",
      "\n",
      "epoch = 2000 | err_diff = 0.019751874832309113\n",
      "error =  2546.455903037343 || [ 0.17979616 -0.13474108 -0.13580321  1.29392419  0.15625886 -0.4651693\n",
      "  0.26716701  0.73993177 -0.39257773 -0.31338103  0.19791583 -0.54599206\n",
      "  0.2711615   0.00449318  1.56653593  0.43290789  0.19597455 -2.13650671\n",
      "  1.5306975  -0.10347338 -0.00805733  1.14401749 -1.29757925 -1.60420378\n",
      "  0.73678367 -0.54925321 -0.50407343 -0.27766831]\n",
      "score = [4.285676525422155, 0.11423450121487416]\n",
      "\n",
      "epoch = 2500 | err_diff = 0.014887912162521388\n",
      "error =  2537.8561283891236 || [ 0.17342905 -0.12390005 -0.13091316  1.29244315  0.14809631 -0.48020712\n",
      "  0.24517758  0.73750758 -0.38227953 -0.30865418  0.19639959 -0.55412641\n",
      "  0.25618925  0.00254963  1.5763858   0.43750614  0.19444268 -2.14460006\n",
      "  1.52923091 -0.09398591 -0.00355522  1.14248857 -1.29857258 -1.59500173\n",
      "  0.74122044 -0.54977703 -0.49508762 -0.2777264 ]\n",
      "score = [5.00863794440286, 0.11380225827702757]\n",
      "\n",
      "epoch = 3000 | err_diff = 0.011231199682242732\n",
      "error =  2531.3715868091163 || [ 1.67917223e-01 -1.14700636e-01 -1.26746179e-01  1.29114888e+00\n",
      "  1.41007300e-01 -4.93320523e-01  2.25916692e-01  7.35223927e-01\n",
      " -3.73496513e-01 -3.04574123e-01  1.95129639e-01 -5.61136308e-01\n",
      "  2.43186740e-01  7.51612015e-04  1.58483873e+00  4.41531528e-01\n",
      "  1.93215455e-01 -2.15151874e+00  1.52791487e+00 -8.57847026e-02\n",
      "  4.45762687e-04  1.14132158e+00 -1.29941036e+00 -1.58698206e+00\n",
      "  7.45225680e-01 -5.50140303e-01 -4.87186408e-01 -2.77618952e-01]\n",
      "score = [5.575385514204767, 0.11346226081607776]\n",
      "\n",
      "epoch = 3500 | err_diff = 0.008482077085773199\n",
      "error =  2526.4772755847425 || [ 1.63146956e-01 -1.06924416e-01 -1.23206006e-01  1.29001663e+00\n",
      "  1.34849173e-01 -5.04765297e-01  2.09021663e-01  7.33062315e-01\n",
      " -3.66027130e-01 -3.01054809e-01  1.94073231e-01 -5.67171256e-01\n",
      "  2.31892092e-01 -9.20086312e-04  1.59208047e+00  4.45060204e-01\n",
      "  1.92252315e-01 -2.15741899e+00  1.52672941e+00 -7.86987896e-02\n",
      "  4.01214832e-03  1.14046827e+00 -1.30011329e+00 -1.57998767e+00\n",
      "  7.48856607e-01 -5.50364466e-01 -4.80225780e-01 -2.77368142e-01]\n",
      "score = [6.021706051636477, 0.11319378992793992]\n",
      "\n",
      "epoch = 4000 | err_diff = 0.006415264944280352\n",
      "error =  2522.7785067180403 || [ 0.15901966 -0.10038199 -0.12020918  1.28902495  0.12949823 -0.51476322\n",
      "  0.19417803  0.73100672 -0.35969662 -0.29802163  0.193202   -0.57236088\n",
      "  0.22207824 -0.00248213  1.5982721   0.44815819  0.19151809 -2.16243629\n",
      "  1.52565723 -0.07257992  0.00720163  1.1398868  -1.30069932 -1.57388238\n",
      "  0.75216281 -0.5504681  -0.47408088 -0.27699319]\n",
      "score = [6.374901454042769, 0.11298088352862147]\n",
      "\n",
      "epoch = 4500 | err_diff = 0.004861402114329394\n",
      "error =  2519.9785605504376 || [ 0.15544983 -0.09490914 -0.11768334  1.28815523  0.12484723 -0.5235066\n",
      "  0.18111313  0.72904325 -0.35435344 -0.29540991  0.19249134 -0.57681757\n",
      "  0.2135483  -0.00394898  1.60355328  0.45088272  0.1909823  -2.1666881\n",
      "  1.52468333 -0.06729957  0.01006421  1.13954092 -1.301184   -1.56854808\n",
      "  0.75518731 -0.55046732 -0.46864346 -0.27651075]\n",
      "score = [6.655841765598824, 0.11281124562127715]\n",
      "\n",
      "epoch = 5000 | err_diff = 0.0036931658182766114\n",
      "error =  2517.854368319659 || [ 0.15236337 -0.09036349 -0.11556574  1.28739136  0.12080316 -0.53116215\n",
      "  0.16959057  0.72715985 -0.34986623 -0.29316354  0.19191991 -0.58063877\n",
      "  0.20613157 -0.00533316  1.60804508  0.45328344  0.1906185  -2.17027627\n",
      "  1.52379469 -0.06274628  0.01264328  1.13939916 -1.30158083 -1.56388238\n",
      "  0.75796737 -0.5503761  -0.46381964 -0.27593529]\n",
      "score = [6.88051570514796, 0.11267539878696783]\n",
      "\n",
      "epoch = 5500 | err_diff = 0.0028148370015514956\n",
      "error =  2516.2382307283174 || [ 0.14969599 -0.08662158 -0.11380198  1.28671934  0.11728542 -0.53787445\n",
      "  0.15940535  0.72534608 -0.34612106 -0.29123391  0.19146923 -0.58390898\n",
      "  0.19968005 -0.00664553  1.6118525   0.45540338  0.19040377 -2.17328911\n",
      "  1.52298003 -0.05882342  0.01497643  1.13943426 -1.3019015  -1.55979651\n",
      "  0.76053533 -0.55020655 -0.45952804 -0.27527932]\n",
      "score = [7.0612024523776356, 0.11256602942647022]\n",
      "\n",
      "epoch = 6000 | err_diff = 0.0021544557484958204\n",
      "error =  2515.0040742812794 || [ 0.14739197 -0.08357637 -0.11234488  1.28612705  0.11422408 -0.54376884\n",
      "  0.15037975  0.72359287 -0.34301917 -0.28957884  0.19112326 -0.58670145\n",
      "  0.19406539 -0.00789552  1.61506658  0.45727986  0.19031824 -2.17580319\n",
      "  1.52222957 -0.05544721  0.01709626  1.13962257 -1.30215615 -1.55621349\n",
      "  0.76291922 -0.54996917 -0.4556981  -0.27455372]\n",
      "score = [7.207357886143139, 0.11247748400445264]\n",
      "\n",
      "epoch = 6500 | err_diff = 0.0016579238390477258\n",
      "error =  2514.0571190387914 || [ 0.145403   -0.08113505 -0.11115356  1.28560397  0.11155853 -0.548954\n",
      "  0.14235968  0.72189238 -0.34047491 -0.28816178  0.19086809 -0.58907971\n",
      "  0.18917633 -0.00909129  1.61776624  0.45894522  0.19034466 -2.17788489\n",
      "  1.52153484 -0.05254501  0.01903106  1.1399436  -1.30235358 -1.55306657\n",
      "  0.76514335 -0.54967307 -0.45226862 -0.27376789]\n",
      "score = [7.326285445260949, 0.1124053825903709]\n",
      "\n",
      "epoch = 7000 | err_diff = 0.0012845711680711247\n",
      "error =  2513.3261110127337 || [ 0.14368719 -0.07921715 -0.11019255  1.28514099  0.10923622 -0.55352417\n",
      "  0.13521151  0.72023779 -0.33841403 -0.28695102  0.19069167 -0.59109884\n",
      "  0.18491639 -0.01023993  1.62001992  0.46042752  0.19046811 -2.17959177\n",
      "  1.52088847 -0.05005381  0.02080532  1.14037957 -1.30250142 -1.55029787\n",
      "  0.76722879 -0.54932614 -0.44918656 -0.27293001]\n",
      "score = [7.423644458860412, 0.11234632294776016]\n",
      "\n",
      "epoch = 7500 | err_diff = 0.001003822665552434\n",
      "error =  2512.7574826169985 || [ 0.1422082  -0.0777529  -0.1094311   1.28473021  0.10721157 -0.55756106\n",
      "  0.1288194   0.71862319 -0.33677215 -0.28591909  0.19058353 -0.59280658\n",
      "  0.18120192 -0.01134757  1.62188695  0.46175108  0.19067562 -2.18097375\n",
      "  1.52028409 -0.04791902  0.0224403   1.14091508 -1.30260626 -1.54785724\n",
      "  0.76919382 -0.54893523 -0.44640592 -0.27204716]\n",
      "score = [7.503835757612309, 0.11229765420657858]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8000 | err_diff = 0.0007926925154606579\n",
      "error =  2512.310962595536 || [ 0.14093455 -0.0766818  -0.10884255  1.28436477  0.10544507 -0.56113549\n",
      "  0.1230829   0.71704348 -0.33549345 -0.28504214  0.19053463 -0.59424435\n",
      "  0.17796038 -0.01241955  1.62341879  0.46293698  0.19095597 -2.1820741\n",
      "  1.51971617 -0.04609327  0.02395442  1.14153675 -1.30267386 -1.54570115\n",
      "  0.77105427 -0.54850627 -0.44388681 -0.27112547]\n",
      "score = [7.570294566093394, 0.11225730383413503]\n",
      "\n",
      "epoch = 8500 | err_diff = 0.0006339003821267397\n",
      "error =  2511.956275642471 || [ 0.13983893 -0.07595139 -0.1084038   1.28403872  0.10390247 -0.56430891\n",
      "  0.11791491  0.7154942  -0.33452952 -0.28429953  0.19053709 -0.59544804\n",
      "  0.17512889 -0.01346047  1.62466005  0.46400353  0.19129941 -2.18293034\n",
      "  1.51917992 -0.04453552  0.02536368  1.14223301 -1.30270917 -1.54379189\n",
      "  0.77282389 -0.54804441 -0.44159465 -0.27017026]\n",
      "score = [7.625713291361313, 0.11222364533957116]\n",
      "\n",
      "epoch = 9000 | err_diff = 0.0005144552133060643\n",
      "error =  2511.6706612338494 || [ 0.13889766 -0.0755162  -0.1080948   1.28374693  0.10255405 -0.56713457\n",
      "  0.11323993  0.7139715  -0.3338384  -0.28367333  0.19058412 -0.59644878\n",
      "  0.17265293 -0.01447434  1.6256494   0.46496656  0.1916975  -2.183575\n",
      "  1.51867118 -0.04321015  0.02668191  1.1429938  -1.3027165  -1.54209674\n",
      "  0.77451458 -0.54755412 -0.43949946 -0.26918616]\n",
      "score = [7.672211262634865, 0.11219539706372737]\n",
      "\n",
      "9072 2511.6341557646247\n",
      "[ 0.13877189 -0.07547493 -0.10805947  1.28370691  0.10237184 -0.56752086\n",
      "  0.11259459  0.7137512  -0.33375808 -0.28359068  0.19059434 -0.59657957\n",
      "  0.17231826 -0.01462032  1.62577484  0.46509935  0.19175974 -2.18365316\n",
      "  1.51859899 -0.04303419  0.0268675   1.14310974 -1.30271546 -1.54186542\n",
      "  0.77475546 -0.5474804  -0.43920844 -0.26904031] [7.678358766308668, 0.11219166182265607]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"junk\", \"lat\", \"lon\", \"alt\"]\n",
    "raw_df = pd.read_csv(\"3D_spatial_network.txt\", sep=',', header=None,\n",
    "                     names=columns).drop(\"junk\", 1)\n",
    "\n",
    "deg = input(\"Enter the Degree of the Polynomial:\")\n",
    "pre_processor = UnlimitedDataWorks(deg=int(deg))\n",
    "X_train, Y_train, x_val, y_val, x_test, y_test = pre_processor.train_test_split(raw_df)\n",
    "\n",
    "model = RegressionModel(N=pre_processor.count,\n",
    "                        X=X_train,\n",
    "                        Y=Y_train,\n",
    "                        x=x_test,\n",
    "                        y=y_test,\n",
    "                        xval=x_val,\n",
    "                        yval=y_val)\n",
    "\n",
    "model.fit()\n",
    "model.gradient_descent()\n",
    "# model.gradient_descent_L1_reg()\n",
    "# model.gradient_descent_L2_reg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
